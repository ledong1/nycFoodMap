{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shile\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n",
      "C:\\Users\\shile\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\msgpack_numpy.py:184: DeprecationWarning: encoding is deprecated, Use raw=False instead.\n",
      "  return _unpackb(packed, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_lg\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "import spacy  # For preprocessing\n",
    "# from pyLDAvis import gensim\n",
    "from time import time \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import scipy.stats as stats\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pyLDAvis\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "b=np.load('ny.eater-content-new.npy')\n",
    "content=b.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "\n",
    "new_content = []\n",
    "for article in content:\n",
    "    if article != []:\n",
    "        text = \"\"\n",
    "        for sent in article:\n",
    "            text = text + sent\n",
    "        new_content.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.41 mins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(395, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', re.sub(\"'s\", '', re.sub(\"’s\", '',str(article)))).lower() for article in new_content)\n",
    "\n",
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:44:34: collecting all words and their counts\n",
      "INFO - 17:44:34: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 17:44:34: collected 239572 word types from a corpus of 283344 words (unigram + bigrams) and 395 sentences\n",
      "INFO - 17:44:34: using 239572 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row.split() for row in df_clean['clean']]\n",
    "bigram = Phrases(sent, min_count=30, progress_per=10000)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:44:34: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO - 17:44:36: built Dictionary(22942 unique tokens: ['abalone', 'absolutely', 'absurd', 'accessible', 'acidity']...) from 395 documents (total 275313 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "doc_list=sentences\n",
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:44:37: using autotuned alpha, starting with [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "INFO - 17:44:37: using symmetric eta at 0.05\n",
      "INFO - 17:44:37: using serial LDA version on this node\n",
      "INFO - 17:44:37: running online (multi-pass) LDA training, 20 topics, 10 passes over the supplied corpus of 395 documents, updating model once every 395 documents, evaluating perplexity every 395 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO - 17:44:41: -13.429 per-word bound, 11032.3 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:44:41: PROGRESS: pass 0, at document #395/395\n",
      "INFO - 17:44:43: optimized alpha [0.09346892, 0.09309897, 0.09423925, 0.094810836, 0.0932745, 0.09369795, 0.08834974, 0.090643354, 0.09448183, 0.09388143, 0.08897023, 0.09605072, 0.0954025, 0.09496476, 0.09458937, 0.09035362, 0.09659594, 0.09462388, 0.094126776, 0.094926566]\n",
      "INFO - 17:44:43: topic #6 (0.088): 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"restaurant\" + 0.004*\"good\" + 0.003*\"chicken\" + 0.003*\"menu\" + 0.003*\"food\" + 0.003*\"chef\" + 0.002*\"flavor\" + 0.002*\"beef\"\n",
      "INFO - 17:44:43: topic #10 (0.089): 0.006*\"restaurant\" + 0.005*\"dish\" + 0.004*\"menu\" + 0.004*\"good\" + 0.003*\"like\" + 0.003*\"flavor\" + 0.003*\"food\" + 0.003*\"beer\" + 0.002*\"meal\" + 0.002*\"chef\"\n",
      "INFO - 17:44:43: topic #12 (0.095): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.003*\"food\" + 0.003*\"sauce\" + 0.003*\"flavor\" + 0.003*\"meat\" + 0.003*\"sweet\"\n",
      "INFO - 17:44:43: topic #11 (0.096): 0.010*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"include\" + 0.003*\"city\" + 0.003*\"food\" + 0.003*\"come\" + 0.003*\"beef\"\n",
      "INFO - 17:44:43: topic #16 (0.097): 0.007*\"restaurant\" + 0.007*\"like\" + 0.005*\"dish\" + 0.005*\"menu\" + 0.004*\"good\" + 0.004*\"chef\" + 0.004*\"food\" + 0.004*\"bar\" + 0.003*\"flavor\" + 0.003*\"meal\"\n",
      "INFO - 17:44:43: topic diff=6.462109, rho=1.000000\n",
      "INFO - 17:44:47: -10.080 per-word bound, 1082.3 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:44:47: PROGRESS: pass 1, at document #395/395\n",
      "INFO - 17:44:49: optimized alpha [0.06492295, 0.06437526, 0.070350856, 0.06959784, 0.06376799, 0.06534987, 0.059084937, 0.060231965, 0.06794582, 0.06628595, 0.059448414, 0.10114675, 0.08822112, 0.073108904, 0.06971819, 0.059731126, 0.13227943, 0.06350554, 0.07199989, 0.0805453]\n",
      "INFO - 17:44:49: topic #6 (0.059): 0.005*\"like\" + 0.004*\"dish\" + 0.004*\"restaurant\" + 0.003*\"good\" + 0.003*\"chicken\" + 0.003*\"menu\" + 0.002*\"curry\" + 0.002*\"food\" + 0.002*\"flavor\" + 0.002*\"beef\"\n",
      "INFO - 17:44:49: topic #10 (0.059): 0.006*\"beer\" + 0.005*\"restaurant\" + 0.004*\"dish\" + 0.003*\"like\" + 0.003*\"menu\" + 0.003*\"good\" + 0.003*\"flavor\" + 0.002*\"lager\" + 0.002*\"food\" + 0.002*\"yeast\"\n",
      "INFO - 17:44:49: topic #12 (0.088): 0.007*\"restaurant\" + 0.006*\"like\" + 0.004*\"dish\" + 0.004*\"good\" + 0.004*\"food\" + 0.003*\"chef\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"flavor\" + 0.003*\"sweet\"\n",
      "INFO - 17:44:49: topic #11 (0.101): 0.010*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + 0.004*\"menu\" + 0.004*\"good\" + 0.004*\"food\" + 0.004*\"include\" + 0.003*\"chicken\" + 0.003*\"open\" + 0.003*\"come\"\n",
      "INFO - 17:44:49: topic #16 (0.132): 0.007*\"restaurant\" + 0.007*\"like\" + 0.005*\"dish\" + 0.004*\"menu\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"food\" + 0.003*\"bar\" + 0.003*\"come\" + 0.003*\"meal\"\n",
      "INFO - 17:44:49: topic diff=2.569042, rho=0.577350\n",
      "INFO - 17:44:53: -9.446 per-word bound, 697.7 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:44:53: PROGRESS: pass 2, at document #395/395\n",
      "INFO - 17:44:55: optimized alpha [0.051363748, 0.051263936, 0.056003965, 0.05526678, 0.05071701, 0.052073263, 0.04720276, 0.04799167, 0.054549433, 0.052578293, 0.04742788, 0.09151191, 0.0716556, 0.05861663, 0.055850375, 0.04748709, 0.1533859, 0.049990404, 0.05728626, 0.06615648]\n",
      "INFO - 17:44:55: topic #6 (0.047): 0.004*\"curry\" + 0.004*\"like\" + 0.004*\"sri\" + 0.003*\"dish\" + 0.003*\"lankan\" + 0.003*\"restaurant\" + 0.003*\"chicken\" + 0.002*\"good\" + 0.002*\"hopper\" + 0.002*\"san\"\n",
      "INFO - 17:44:55: topic #10 (0.047): 0.009*\"beer\" + 0.004*\"lager\" + 0.004*\"yeast\" + 0.003*\"like\" + 0.003*\"restaurant\" + 0.003*\"flavor\" + 0.003*\"dish\" + 0.003*\"brett\" + 0.002*\"menu\" + 0.002*\"good\"\n",
      "INFO - 17:44:55: topic #12 (0.072): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"chef\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"flavor\"\n",
      "INFO - 17:44:55: topic #11 (0.092): 0.010*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + 0.004*\"menu\" + 0.004*\"food\" + 0.004*\"good\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.003*\"open\" + 0.003*\"fry\"\n",
      "INFO - 17:44:55: topic #16 (0.153): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"menu\" + 0.004*\"good\" + 0.004*\"food\" + 0.003*\"bar\" + 0.003*\"come\" + 0.003*\"flavor\"\n",
      "INFO - 17:44:55: topic diff=1.973994, rho=0.500000\n",
      "INFO - 17:44:58: -9.171 per-word bound, 576.4 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:44:58: PROGRESS: pass 3, at document #395/395\n",
      "INFO - 17:45:00: optimized alpha [0.043588668, 0.04366107, 0.04772123, 0.047067735, 0.04321062, 0.044514764, 0.040300537, 0.04087253, 0.046836637, 0.04457786, 0.040469505, 0.084448725, 0.061801065, 0.050127216, 0.047776613, 0.040378697, 0.16550867, 0.042264983, 0.048848115, 0.05751995]\n",
      "INFO - 17:45:00: topic #6 (0.040): 0.006*\"curry\" + 0.005*\"sri\" + 0.004*\"lankan\" + 0.003*\"hopper\" + 0.003*\"san\" + 0.003*\"like\" + 0.003*\"chicken\" + 0.003*\"rasa\" + 0.003*\"dish\" + 0.003*\"restaurant\"\n",
      "INFO - 17:45:00: topic #15 (0.040): 0.004*\"tajine\" + 0.003*\"restaurant\" + 0.003*\"chicken\" + 0.003*\"like\" + 0.002*\"menu\" + 0.002*\"couscous\" + 0.002*\"omar\" + 0.002*\"dish\" + 0.002*\"french\" + 0.002*\"serve\"\n",
      "INFO - 17:45:00: topic #12 (0.062): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"chef\" + 0.003*\"sweet\" + 0.003*\"come\"\n",
      "INFO - 17:45:00: topic #11 (0.084): 0.010*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + 0.004*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"good\" + 0.004*\"chicken\" + 0.003*\"open\" + 0.003*\"fry\"\n",
      "INFO - 17:45:00: topic #16 (0.166): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"menu\" + 0.004*\"good\" + 0.003*\"food\" + 0.003*\"come\" + 0.003*\"bar\" + 0.003*\"flavor\"\n",
      "INFO - 17:45:00: topic diff=1.462984, rho=0.447214\n",
      "INFO - 17:45:03: -9.033 per-word bound, 523.7 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:45:03: PROGRESS: pass 4, at document #395/395\n",
      "INFO - 17:45:05: optimized alpha [0.0384212, 0.03863178, 0.04229457, 0.041575275, 0.038186736, 0.0394522, 0.035632882, 0.036081027, 0.041719586, 0.03933314, 0.03581537, 0.07942124, 0.055311207, 0.044542924, 0.042405512, 0.035598993, 0.17318812, 0.037140522, 0.043275673, 0.051898982]\n",
      "INFO - 17:45:05: topic #15 (0.036): 0.006*\"tajine\" + 0.003*\"couscous\" + 0.003*\"omar\" + 0.003*\"chicken\" + 0.002*\"restaurant\" + 0.002*\"menu\" + 0.002*\"like\" + 0.002*\"algerian\" + 0.002*\"bastilla\" + 0.002*\"french\"\n",
      "INFO - 17:45:05: topic #6 (0.036): 0.007*\"curry\" + 0.006*\"sri\" + 0.005*\"lankan\" + 0.004*\"hopper\" + 0.004*\"san\" + 0.004*\"rasa\" + 0.003*\"chicken\" + 0.003*\"like\" + 0.003*\"roti\" + 0.003*\"dish\"\n",
      "INFO - 17:45:05: topic #12 (0.055): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"chef\" + 0.003*\"come\"\n",
      "INFO - 17:45:05: topic #11 (0.079): 0.010*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + 0.004*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + 0.003*\"open\" + 0.003*\"fry\"\n",
      "INFO - 17:45:05: topic #16 (0.173): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"come\" + 0.003*\"bar\" + 0.003*\"flavor\"\n",
      "INFO - 17:45:05: topic diff=1.077901, rho=0.408248\n",
      "INFO - 17:45:08: -8.956 per-word bound, 496.6 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:45:08: PROGRESS: pass 5, at document #395/395\n",
      "INFO - 17:45:11: optimized alpha [0.034675043, 0.03496996, 0.03836797, 0.037588008, 0.034528967, 0.035804752, 0.032211952, 0.032580145, 0.03798618, 0.03553599, 0.032402504, 0.076060496, 0.050551098, 0.04055333, 0.0385097, 0.03210888, 0.17796859, 0.03346586, 0.03925233, 0.047827795]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:45:11: topic #15 (0.032): 0.007*\"tajine\" + 0.004*\"couscous\" + 0.003*\"omar\" + 0.002*\"chicken\" + 0.002*\"algerian\" + 0.002*\"bastilla\" + 0.002*\"north\" + 0.002*\"french\" + 0.002*\"restaurant\" + 0.002*\"african\"\n",
      "INFO - 17:45:11: topic #6 (0.032): 0.008*\"curry\" + 0.007*\"sri\" + 0.006*\"lankan\" + 0.004*\"hopper\" + 0.004*\"san\" + 0.004*\"rasa\" + 0.004*\"chicken\" + 0.003*\"fish\" + 0.003*\"roti\" + 0.003*\"joe\"\n",
      "INFO - 17:45:11: topic #12 (0.051): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"come\" + 0.003*\"chef\"\n",
      "INFO - 17:45:11: topic #11 (0.076): 0.010*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + 0.004*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + 0.003*\"open\" + 0.003*\"fry\"\n",
      "INFO - 17:45:11: topic #16 (0.178): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"come\" + 0.003*\"bar\" + 0.003*\"flavor\"\n",
      "INFO - 17:45:11: topic diff=0.796076, rho=0.377964\n",
      "INFO - 17:45:14: -8.910 per-word bound, 481.1 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:45:14: PROGRESS: pass 6, at document #395/395\n",
      "INFO - 17:45:16: optimized alpha [0.031804647, 0.032185752, 0.03540025, 0.03454696, 0.031717926, 0.032997604, 0.029570624, 0.029883025, 0.035113275, 0.032662902, 0.029765947, 0.073775895, 0.046911392, 0.037500676, 0.03552392, 0.02942127, 0.18101957, 0.030651432, 0.036178432, 0.044760622]\n",
      "INFO - 17:45:16: topic #15 (0.029): 0.008*\"tajine\" + 0.004*\"couscous\" + 0.004*\"omar\" + 0.003*\"algerian\" + 0.003*\"bastilla\" + 0.002*\"north\" + 0.002*\"chicken\" + 0.002*\"french\" + 0.002*\"african\" + 0.002*\"feature\"\n",
      "INFO - 17:45:16: topic #6 (0.030): 0.009*\"curry\" + 0.007*\"sri\" + 0.006*\"lankan\" + 0.005*\"hopper\" + 0.005*\"san\" + 0.004*\"rasa\" + 0.004*\"chicken\" + 0.003*\"joe\" + 0.003*\"fish\" + 0.003*\"roti\"\n",
      "INFO - 17:45:16: topic #12 (0.047): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"come\" + 0.003*\"chef\"\n",
      "INFO - 17:45:16: topic #11 (0.074): 0.010*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + 0.004*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + 0.003*\"fry\" + 0.003*\"open\"\n",
      "INFO - 17:45:16: topic #16 (0.181): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"bar\" + 0.003*\"come\" + 0.003*\"flavor\"\n",
      "INFO - 17:45:16: topic diff=0.591443, rho=0.353553\n",
      "INFO - 17:45:19: -8.882 per-word bound, 471.6 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:45:19: PROGRESS: pass 7, at document #395/395\n",
      "INFO - 17:45:21: optimized alpha [0.029517459, 0.029961638, 0.03303481, 0.032131743, 0.02947306, 0.03075272, 0.02745392, 0.027751222, 0.03284836, 0.030399736, 0.027677242, 0.07199114, 0.04408458, 0.035070125, 0.033144258, 0.027271837, 0.18275215, 0.028408466, 0.03373425, 0.042347524]\n",
      "INFO - 17:45:21: topic #15 (0.027): 0.008*\"tajine\" + 0.005*\"couscous\" + 0.004*\"omar\" + 0.003*\"algerian\" + 0.003*\"bastilla\" + 0.003*\"north\" + 0.002*\"african\" + 0.002*\"french\" + 0.002*\"chicken\" + 0.002*\"brick\"\n",
      "INFO - 17:45:21: topic #6 (0.027): 0.009*\"curry\" + 0.007*\"sri\" + 0.006*\"lankan\" + 0.005*\"hopper\" + 0.005*\"san\" + 0.004*\"rasa\" + 0.004*\"joe\" + 0.004*\"chicken\" + 0.003*\"fish\" + 0.003*\"roti\"\n",
      "INFO - 17:45:21: topic #12 (0.044): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"come\" + 0.003*\"chef\"\n",
      "INFO - 17:45:21: topic #11 (0.072): 0.010*\"restaurant\" + 0.006*\"dish\" + 0.006*\"like\" + 0.004*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + 0.003*\"fry\" + 0.003*\"open\"\n",
      "INFO - 17:45:21: topic #16 (0.183): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"bar\" + 0.003*\"come\" + 0.003*\"flavor\"\n",
      "INFO - 17:45:21: topic diff=0.442453, rho=0.333333\n",
      "INFO - 17:45:24: -8.863 per-word bound, 465.5 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:45:24: PROGRESS: pass 8, at document #395/395\n",
      "INFO - 17:45:26: optimized alpha [0.027642304, 0.028133204, 0.031094946, 0.030156674, 0.027629446, 0.028906878, 0.02571074, 0.025996082, 0.030985594, 0.028594593, 0.02597907, 0.07066623, 0.04180754, 0.033108547, 0.031192984, 0.025504528, 0.18386951, 0.026569184, 0.031733748, 0.040362522]\n",
      "INFO - 17:45:26: topic #15 (0.026): 0.009*\"tajine\" + 0.005*\"couscous\" + 0.004*\"omar\" + 0.003*\"algerian\" + 0.003*\"bastilla\" + 0.003*\"north\" + 0.002*\"african\" + 0.002*\"french\" + 0.002*\"merguez\" + 0.002*\"brick\"\n",
      "INFO - 17:45:26: topic #6 (0.026): 0.009*\"curry\" + 0.007*\"sri\" + 0.006*\"lankan\" + 0.005*\"hopper\" + 0.005*\"san\" + 0.004*\"rasa\" + 0.004*\"joe\" + 0.004*\"chicken\" + 0.003*\"fish\" + 0.003*\"roti\"\n",
      "INFO - 17:45:26: topic #12 (0.042): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"come\" + 0.003*\"chef\"\n",
      "INFO - 17:45:26: topic #11 (0.071): 0.010*\"restaurant\" + 0.006*\"dish\" + 0.006*\"like\" + 0.005*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + 0.003*\"fry\" + 0.003*\"open\"\n",
      "INFO - 17:45:26: topic #16 (0.184): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"bar\" + 0.003*\"flavor\" + 0.003*\"come\"\n",
      "INFO - 17:45:26: topic diff=0.333681, rho=0.316228\n",
      "INFO - 17:45:29: -8.849 per-word bound, 461.3 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:45:29: PROGRESS: pass 9, at document #395/395\n",
      "INFO - 17:45:31: optimized alpha [0.026070138, 0.026617436, 0.029468056, 0.028528355, 0.026081614, 0.02735555, 0.02424392, 0.024519484, 0.029419439, 0.027077232, 0.0245477, 0.06963627, 0.039894495, 0.03146357, 0.029556774, 0.024019387, 0.18454117, 0.025026815, 0.030057885, 0.03869293]\n",
      "INFO - 17:45:31: topic #15 (0.024): 0.009*\"tajine\" + 0.005*\"couscous\" + 0.004*\"omar\" + 0.003*\"algerian\" + 0.003*\"bastilla\" + 0.003*\"north\" + 0.002*\"african\" + 0.002*\"merguez\" + 0.002*\"french\" + 0.002*\"brick\"\n",
      "INFO - 17:45:31: topic #6 (0.024): 0.010*\"curry\" + 0.008*\"sri\" + 0.006*\"lankan\" + 0.005*\"hopper\" + 0.005*\"san\" + 0.004*\"rasa\" + 0.004*\"joe\" + 0.004*\"chicken\" + 0.004*\"fish\" + 0.003*\"roti\"\n",
      "INFO - 17:45:31: topic #12 (0.040): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"come\" + 0.003*\"chef\"\n",
      "INFO - 17:45:31: topic #11 (0.070): 0.010*\"restaurant\" + 0.006*\"dish\" + 0.006*\"like\" + 0.005*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + 0.003*\"fry\" + 0.003*\"rice\"\n",
      "INFO - 17:45:31: topic #16 (0.185): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"bar\" + 0.003*\"flavor\" + 0.003*\"come\"\n",
      "INFO - 17:45:31: topic diff=0.253710, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:45:31: topic #0 (0.026): 0.007*\"restaurant\" + 0.005*\"not\" + 0.005*\"good\" + 0.005*\"like\" + 0.004*\"spot\" + 0.004*\"cardoz\" + 0.004*\"carbone\" + 0.004*\"white\" + 0.004*\"beef\" + 0.003*\"bread\" + 0.003*\"rice\" + 0.003*\"meat\" + 0.003*\"flavor\" + 0.003*\"food\" + 0.003*\"dish\" + 0.003*\"sauce\" + 0.003*\"steak\" + 0.003*\"new_york\" + 0.003*\"steakhouse\" + 0.003*\"chicken\"\n",
      "INFO - 17:45:31: topic #1 (0.027): 0.010*\"like\" + 0.009*\"drink\" + 0.008*\"chicken\" + 0.008*\"sample_dish\" + 0.007*\"sauce\" + 0.006*\"salad\" + 0.006*\"dish\" + 0.006*\"sandwich\" + 0.006*\"cheese\" + 0.005*\"beer\" + 0.005*\"pizza\" + 0.005*\"fry\" + 0.005*\"noodle\" + 0.005*\"entree\" + 0.005*\"shrimp\" + 0.005*\"cream\" + 0.005*\"selection\" + 0.004*\"menu\" + 0.004*\"con\" + 0.004*\"garlic\"\n",
      "INFO - 17:45:31: topic #2 (0.029): 0.007*\"pizza\" + 0.006*\"like\" + 0.005*\"pie\" + 0.005*\"restaurant\" + 0.005*\"wine\" + 0.005*\"menu\" + 0.004*\"open\" + 0.004*\"good\" + 0.004*\"chef\" + 0.004*\"expect\" + 0.003*\"come\" + 0.003*\"salad\" + 0.003*\"cheese\" + 0.003*\"red\" + 0.003*\"bar\" + 0.003*\"player\" + 0.003*\"sauce\" + 0.003*\"dish\" + 0.003*\"white\" + 0.003*\"emmy\"\n",
      "INFO - 17:45:31: topic #3 (0.029): 0.004*\"dish\" + 0.004*\"taco\" + 0.004*\"price\" + 0.004*\"restaurant\" + 0.004*\"chicken\" + 0.003*\"chef\" + 0.003*\"bar\" + 0.003*\"good\" + 0.003*\"menu\" + 0.003*\"crenn\" + 0.003*\"couscous\" + 0.003*\"fuku\" + 0.003*\"like\" + 0.003*\"place\" + 0.003*\"order\" + 0.003*\"new\" + 0.002*\"small\" + 0.002*\"m\" + 0.002*\"nachos\" + 0.002*\"food\"\n",
      "INFO - 17:45:31: topic #4 (0.026): 0.009*\"chicken\" + 0.005*\"like\" + 0.005*\"rice\" + 0.005*\"hot\" + 0.004*\"di\" + 0.004*\"vietnamese\" + 0.004*\"restaurant\" + 0.004*\"dish\" + 0.004*\"chef\" + 0.003*\"flavor\" + 0.003*\"city\" + 0.003*\"meal\" + 0.003*\"new_york\" + 0.003*\"sauce\" + 0.003*\"serve\" + 0.003*\"crab\" + 0.003*\"b\" + 0.003*\"tasting\" + 0.003*\"beef\" + 0.003*\"noodle\"\n",
      "INFO - 17:45:31: topic #5 (0.027): 0.012*\"ipa\" + 0.007*\"beer\" + 0.007*\"like\" + 0.006*\"hop\" + 0.005*\"brewery\" + 0.004*\"new\" + 0.003*\"restaurant\" + 0.003*\"double\" + 0.003*\"good\" + 0.003*\"old\" + 0.003*\"banya\" + 0.003*\"day\" + 0.003*\"style\" + 0.003*\"flavor\" + 0.002*\"dumpling\" + 0.002*\"room\" + 0.002*\"long\" + 0.002*\"ale\" + 0.002*\"pack\" + 0.002*\"soup\"\n",
      "INFO - 17:45:31: topic #6 (0.024): 0.010*\"curry\" + 0.008*\"sri\" + 0.006*\"lankan\" + 0.005*\"hopper\" + 0.005*\"san\" + 0.004*\"rasa\" + 0.004*\"joe\" + 0.004*\"chicken\" + 0.004*\"fish\" + 0.003*\"roti\" + 0.003*\"lamprais\" + 0.003*\"starch\" + 0.003*\"juice\" + 0.003*\"tea\" + 0.003*\"beverage\" + 0.002*\"rice\" + 0.002*\"restaurant\" + 0.002*\"kottu\" + 0.002*\"lamb\" + 0.002*\"dish\"\n",
      "INFO - 17:45:31: topic #7 (0.025): 0.006*\"irani\" + 0.006*\"chai\" + 0.005*\"indian\" + 0.004*\"hamilton\" + 0.004*\"like\" + 0.004*\"curry\" + 0.004*\"prune\" + 0.004*\"bitter\" + 0.003*\"butter\" + 0.003*\"session\" + 0.003*\"pav\" + 0.003*\"roll\" + 0.003*\"american\" + 0.003*\"cost\" + 0.003*\"mango\" + 0.003*\"ipa\" + 0.002*\"style\" + 0.002*\"cup\" + 0.002*\"bun\" + 0.002*\"brun\"\n",
      "INFO - 17:45:31: topic #8 (0.029): 0.007*\"menu\" + 0.006*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + 0.005*\"noodle\" + 0.004*\"pork\" + 0.004*\"beef\" + 0.004*\"chicken\" + 0.004*\"call\" + 0.004*\"good\" + 0.004*\"include\" + 0.003*\"rice\" + 0.003*\"soup\" + 0.003*\"food\" + 0.003*\"sauce\" + 0.003*\"tibetan\" + 0.003*\"tea\" + 0.003*\"green\" + 0.003*\"pizza\" + 0.003*\"indian\"\n",
      "INFO - 17:45:31: topic #9 (0.027): 0.009*\"sushi\" + 0.009*\"restaurant\" + 0.006*\"like\" + 0.005*\"price\" + 0.005*\"fish\" + 0.004*\"meal\" + 0.004*\"new_york\" + 0.004*\"menu\" + 0.004*\"ya\" + 0.004*\"dish\" + 0.003*\"chef\" + 0.003*\"food\" + 0.003*\"o\" + 0.003*\"nakazawa\" + 0.003*\"good\" + 0.003*\"serve\" + 0.003*\"course\" + 0.003*\"cost\" + 0.003*\"posto\" + 0.003*\"del\"\n",
      "INFO - 17:45:31: topic #10 (0.025): 0.020*\"beer\" + 0.009*\"lager\" + 0.007*\"yeast\" + 0.005*\"brett\" + 0.005*\"ale\" + 0.003*\"flavor\" + 0.003*\"like\" + 0.003*\"brewery\" + 0.003*\"create\" + 0.003*\"schlafly\" + 0.003*\"ferment\" + 0.003*\"hop\" + 0.003*\"little\" + 0.003*\"allagash\" + 0.003*\"brew\" + 0.003*\"brewing\" + 0.002*\"festival\" + 0.002*\"unfiltered\" + 0.002*\"ipa\" + 0.002*\"favorite\"\n",
      "INFO - 17:45:31: topic #11 (0.070): 0.010*\"restaurant\" + 0.006*\"dish\" + 0.006*\"like\" + 0.005*\"menu\" + 0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + 0.003*\"fry\" + 0.003*\"rice\" + 0.003*\"open\" + 0.003*\"meat\" + 0.003*\"fish\" + 0.003*\"come\" + 0.003*\"salad\" + 0.003*\"red\" + 0.003*\"serve\" + 0.003*\"beef\" + 0.003*\"new\" + 0.003*\"place\"\n",
      "INFO - 17:45:31: topic #12 (0.040): 0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + 0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"come\" + 0.003*\"chef\" + 0.003*\"chicken\" + 0.003*\"city\" + 0.002*\"fry\" + 0.002*\"cost\" + 0.002*\"spot\" + 0.002*\"sandwich\" + 0.002*\"place\" + 0.002*\"way\" + 0.002*\"menu\" + 0.002*\"beef\"\n",
      "INFO - 17:45:31: topic #13 (0.031): 0.009*\"restaurant\" + 0.006*\"dish\" + 0.005*\"pizza\" + 0.005*\"pasta\" + 0.005*\"like\" + 0.004*\"wine\" + 0.004*\"good\" + 0.004*\"menu\" + 0.004*\"open\" + 0.004*\"include\" + 0.004*\"glass\" + 0.003*\"come\" + 0.003*\"drink\" + 0.003*\"chef\" + 0.003*\"bar\" + 0.003*\"serve\" + 0.003*\"food\" + 0.003*\"italian\" + 0.003*\"sauce\" + 0.003*\"bottle\"\n",
      "INFO - 17:45:31: topic #14 (0.030): 0.009*\"restaurant\" + 0.006*\"dish\" + 0.005*\"like\" + 0.005*\"good\" + 0.004*\"beer\" + 0.004*\"new\" + 0.004*\"food\" + 0.003*\"menu\" + 0.003*\"not\" + 0.003*\"open\" + 0.003*\"pork\" + 0.003*\"thing\" + 0.002*\"nishi\" + 0.002*\"drink\" + 0.002*\"vandal\" + 0.002*\"call\" + 0.002*\"bar\" + 0.002*\"little\" + 0.002*\"cocktail\" + 0.002*\"time\"\n",
      "INFO - 17:45:31: topic #15 (0.024): 0.009*\"tajine\" + 0.005*\"couscous\" + 0.004*\"omar\" + 0.003*\"algerian\" + 0.003*\"bastilla\" + 0.003*\"north\" + 0.002*\"african\" + 0.002*\"merguez\" + 0.002*\"french\" + 0.002*\"brick\" + 0.002*\"feature\" + 0.002*\"warka\" + 0.002*\"lemonwhat\" + 0.001*\"algeria\" + 0.001*\"maghreb\" + 0.001*\"clay\" + 0.001*\"chicken\" + 0.001*\"californian\" + 0.001*\"moroccan\" + 0.001*\"powdered\"\n",
      "INFO - 17:45:31: topic #16 (0.185): 0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + 0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"bar\" + 0.003*\"flavor\" + 0.003*\"come\" + 0.003*\"serve\" + 0.003*\"meal\" + 0.003*\"not\" + 0.003*\"course\" + 0.003*\"new_york\" + 0.002*\"city\" + 0.002*\"time\" + 0.002*\"sweet\" + 0.002*\"kitchen\" + 0.002*\"sauce\"\n",
      "INFO - 17:45:31: topic #17 (0.025): 0.005*\"barbecue\" + 0.004*\"like\" + 0.004*\"meat\" + 0.004*\"serve\" + 0.004*\"qui\" + 0.003*\"pelmeni\" + 0.003*\"franklin\" + 0.003*\"good\" + 0.003*\"pork\" + 0.003*\"austin\" + 0.003*\"frenchette\" + 0.003*\"line\" + 0.003*\"potato\" + 0.003*\"chicken\" + 0.003*\"vegetable\" + 0.002*\"goat\" + 0.002*\"smoke\" + 0.002*\"restaurant\" + 0.002*\"duck\" + 0.002*\"include\"\n",
      "INFO - 17:45:31: topic #18 (0.030): 0.007*\"restaurant\" + 0.007*\"like\" + 0.005*\"dish\" + 0.004*\"good\" + 0.004*\"chef\" + 0.004*\"menu\" + 0.003*\"lamb\" + 0.003*\"food\" + 0.003*\"salad\" + 0.003*\"chinese\" + 0.003*\"table\" + 0.003*\"lunch\" + 0.003*\"not\" + 0.003*\"egg\" + 0.003*\"white\" + 0.003*\"include\" + 0.002*\"open\" + 0.002*\"place\" + 0.002*\"green\" + 0.002*\"come\"\n",
      "INFO - 17:45:31: topic #19 (0.039): 0.006*\"menu\" + 0.006*\"like\" + 0.005*\"dish\" + 0.005*\"restaurant\" + 0.004*\"good\" + 0.004*\"indian\" + 0.004*\"curry\" + 0.004*\"meat\" + 0.004*\"beer\" + 0.003*\"chef\" + 0.003*\"flavor\" + 0.003*\"sauce\" + 0.003*\"city\" + 0.003*\"bar\" + 0.002*\"new\" + 0.002*\"course\" + 0.002*\"food\" + 0.002*\"vegetable\" + 0.002*\"come\" + 0.002*\"room\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.007*\"restaurant\" + 0.005*\"not\" + 0.005*\"good\" + 0.005*\"like\" + '\n",
      "  '0.004*\"spot\" + 0.004*\"cardoz\" + 0.004*\"carbone\" + 0.004*\"white\" + '\n",
      "  '0.004*\"beef\" + 0.003*\"bread\" + 0.003*\"rice\" + 0.003*\"meat\" + 0.003*\"flavor\" '\n",
      "  '+ 0.003*\"food\" + 0.003*\"dish\" + 0.003*\"sauce\" + 0.003*\"steak\" + '\n",
      "  '0.003*\"new_york\" + 0.003*\"steakhouse\" + 0.003*\"chicken\"'),\n",
      " (1,\n",
      "  '0.010*\"like\" + 0.009*\"drink\" + 0.008*\"chicken\" + 0.008*\"sample_dish\" + '\n",
      "  '0.007*\"sauce\" + 0.006*\"salad\" + 0.006*\"dish\" + 0.006*\"sandwich\" + '\n",
      "  '0.006*\"cheese\" + 0.005*\"beer\" + 0.005*\"pizza\" + 0.005*\"fry\" + '\n",
      "  '0.005*\"noodle\" + 0.005*\"entree\" + 0.005*\"shrimp\" + 0.005*\"cream\" + '\n",
      "  '0.005*\"selection\" + 0.004*\"menu\" + 0.004*\"con\" + 0.004*\"garlic\"'),\n",
      " (2,\n",
      "  '0.007*\"pizza\" + 0.006*\"like\" + 0.005*\"pie\" + 0.005*\"restaurant\" + '\n",
      "  '0.005*\"wine\" + 0.005*\"menu\" + 0.004*\"open\" + 0.004*\"good\" + 0.004*\"chef\" + '\n",
      "  '0.004*\"expect\" + 0.003*\"come\" + 0.003*\"salad\" + 0.003*\"cheese\" + '\n",
      "  '0.003*\"red\" + 0.003*\"bar\" + 0.003*\"player\" + 0.003*\"sauce\" + 0.003*\"dish\" + '\n",
      "  '0.003*\"white\" + 0.003*\"emmy\"'),\n",
      " (3,\n",
      "  '0.004*\"dish\" + 0.004*\"taco\" + 0.004*\"price\" + 0.004*\"restaurant\" + '\n",
      "  '0.004*\"chicken\" + 0.003*\"chef\" + 0.003*\"bar\" + 0.003*\"good\" + 0.003*\"menu\" '\n",
      "  '+ 0.003*\"crenn\" + 0.003*\"couscous\" + 0.003*\"fuku\" + 0.003*\"like\" + '\n",
      "  '0.003*\"place\" + 0.003*\"order\" + 0.003*\"new\" + 0.002*\"small\" + 0.002*\"m\" + '\n",
      "  '0.002*\"nachos\" + 0.002*\"food\"'),\n",
      " (4,\n",
      "  '0.009*\"chicken\" + 0.005*\"like\" + 0.005*\"rice\" + 0.005*\"hot\" + 0.004*\"di\" + '\n",
      "  '0.004*\"vietnamese\" + 0.004*\"restaurant\" + 0.004*\"dish\" + 0.004*\"chef\" + '\n",
      "  '0.003*\"flavor\" + 0.003*\"city\" + 0.003*\"meal\" + 0.003*\"new_york\" + '\n",
      "  '0.003*\"sauce\" + 0.003*\"serve\" + 0.003*\"crab\" + 0.003*\"b\" + 0.003*\"tasting\" '\n",
      "  '+ 0.003*\"beef\" + 0.003*\"noodle\"'),\n",
      " (5,\n",
      "  '0.012*\"ipa\" + 0.007*\"beer\" + 0.007*\"like\" + 0.006*\"hop\" + 0.005*\"brewery\" + '\n",
      "  '0.004*\"new\" + 0.003*\"restaurant\" + 0.003*\"double\" + 0.003*\"good\" + '\n",
      "  '0.003*\"old\" + 0.003*\"banya\" + 0.003*\"day\" + 0.003*\"style\" + 0.003*\"flavor\" '\n",
      "  '+ 0.002*\"dumpling\" + 0.002*\"room\" + 0.002*\"long\" + 0.002*\"ale\" + '\n",
      "  '0.002*\"pack\" + 0.002*\"soup\"'),\n",
      " (6,\n",
      "  '0.010*\"curry\" + 0.008*\"sri\" + 0.006*\"lankan\" + 0.005*\"hopper\" + 0.005*\"san\" '\n",
      "  '+ 0.004*\"rasa\" + 0.004*\"joe\" + 0.004*\"chicken\" + 0.004*\"fish\" + '\n",
      "  '0.003*\"roti\" + 0.003*\"lamprais\" + 0.003*\"starch\" + 0.003*\"juice\" + '\n",
      "  '0.003*\"tea\" + 0.003*\"beverage\" + 0.002*\"rice\" + 0.002*\"restaurant\" + '\n",
      "  '0.002*\"kottu\" + 0.002*\"lamb\" + 0.002*\"dish\"'),\n",
      " (7,\n",
      "  '0.006*\"irani\" + 0.006*\"chai\" + 0.005*\"indian\" + 0.004*\"hamilton\" + '\n",
      "  '0.004*\"like\" + 0.004*\"curry\" + 0.004*\"prune\" + 0.004*\"bitter\" + '\n",
      "  '0.003*\"butter\" + 0.003*\"session\" + 0.003*\"pav\" + 0.003*\"roll\" + '\n",
      "  '0.003*\"american\" + 0.003*\"cost\" + 0.003*\"mango\" + 0.003*\"ipa\" + '\n",
      "  '0.002*\"style\" + 0.002*\"cup\" + 0.002*\"bun\" + 0.002*\"brun\"'),\n",
      " (8,\n",
      "  '0.007*\"menu\" + 0.006*\"restaurant\" + 0.006*\"like\" + 0.006*\"dish\" + '\n",
      "  '0.005*\"noodle\" + 0.004*\"pork\" + 0.004*\"beef\" + 0.004*\"chicken\" + '\n",
      "  '0.004*\"call\" + 0.004*\"good\" + 0.004*\"include\" + 0.003*\"rice\" + 0.003*\"soup\" '\n",
      "  '+ 0.003*\"food\" + 0.003*\"sauce\" + 0.003*\"tibetan\" + 0.003*\"tea\" + '\n",
      "  '0.003*\"green\" + 0.003*\"pizza\" + 0.003*\"indian\"'),\n",
      " (9,\n",
      "  '0.009*\"sushi\" + 0.009*\"restaurant\" + 0.006*\"like\" + 0.005*\"price\" + '\n",
      "  '0.005*\"fish\" + 0.004*\"meal\" + 0.004*\"new_york\" + 0.004*\"menu\" + 0.004*\"ya\" '\n",
      "  '+ 0.004*\"dish\" + 0.003*\"chef\" + 0.003*\"food\" + 0.003*\"o\" + 0.003*\"nakazawa\" '\n",
      "  '+ 0.003*\"good\" + 0.003*\"serve\" + 0.003*\"course\" + 0.003*\"cost\" + '\n",
      "  '0.003*\"posto\" + 0.003*\"del\"'),\n",
      " (10,\n",
      "  '0.020*\"beer\" + 0.009*\"lager\" + 0.007*\"yeast\" + 0.005*\"brett\" + 0.005*\"ale\" '\n",
      "  '+ 0.003*\"flavor\" + 0.003*\"like\" + 0.003*\"brewery\" + 0.003*\"create\" + '\n",
      "  '0.003*\"schlafly\" + 0.003*\"ferment\" + 0.003*\"hop\" + 0.003*\"little\" + '\n",
      "  '0.003*\"allagash\" + 0.003*\"brew\" + 0.003*\"brewing\" + 0.002*\"festival\" + '\n",
      "  '0.002*\"unfiltered\" + 0.002*\"ipa\" + 0.002*\"favorite\"'),\n",
      " (11,\n",
      "  '0.010*\"restaurant\" + 0.006*\"dish\" + 0.006*\"like\" + 0.005*\"menu\" + '\n",
      "  '0.004*\"food\" + 0.004*\"include\" + 0.004*\"chicken\" + 0.004*\"good\" + '\n",
      "  '0.003*\"fry\" + 0.003*\"rice\" + 0.003*\"open\" + 0.003*\"meat\" + 0.003*\"fish\" + '\n",
      "  '0.003*\"come\" + 0.003*\"salad\" + 0.003*\"red\" + 0.003*\"serve\" + 0.003*\"beef\" + '\n",
      "  '0.003*\"new\" + 0.003*\"place\"'),\n",
      " (12,\n",
      "  '0.007*\"restaurant\" + 0.006*\"like\" + 0.005*\"good\" + 0.004*\"dish\" + '\n",
      "  '0.004*\"food\" + 0.003*\"sauce\" + 0.003*\"meat\" + 0.003*\"sweet\" + 0.003*\"come\" '\n",
      "  '+ 0.003*\"chef\" + 0.003*\"chicken\" + 0.003*\"city\" + 0.002*\"fry\" + '\n",
      "  '0.002*\"cost\" + 0.002*\"spot\" + 0.002*\"sandwich\" + 0.002*\"place\" + '\n",
      "  '0.002*\"way\" + 0.002*\"menu\" + 0.002*\"beef\"'),\n",
      " (13,\n",
      "  '0.009*\"restaurant\" + 0.006*\"dish\" + 0.005*\"pizza\" + 0.005*\"pasta\" + '\n",
      "  '0.005*\"like\" + 0.004*\"wine\" + 0.004*\"good\" + 0.004*\"menu\" + 0.004*\"open\" + '\n",
      "  '0.004*\"include\" + 0.004*\"glass\" + 0.003*\"come\" + 0.003*\"drink\" + '\n",
      "  '0.003*\"chef\" + 0.003*\"bar\" + 0.003*\"serve\" + 0.003*\"food\" + 0.003*\"italian\" '\n",
      "  '+ 0.003*\"sauce\" + 0.003*\"bottle\"'),\n",
      " (14,\n",
      "  '0.009*\"restaurant\" + 0.006*\"dish\" + 0.005*\"like\" + 0.005*\"good\" + '\n",
      "  '0.004*\"beer\" + 0.004*\"new\" + 0.004*\"food\" + 0.003*\"menu\" + 0.003*\"not\" + '\n",
      "  '0.003*\"open\" + 0.003*\"pork\" + 0.003*\"thing\" + 0.002*\"nishi\" + 0.002*\"drink\" '\n",
      "  '+ 0.002*\"vandal\" + 0.002*\"call\" + 0.002*\"bar\" + 0.002*\"little\" + '\n",
      "  '0.002*\"cocktail\" + 0.002*\"time\"'),\n",
      " (15,\n",
      "  '0.009*\"tajine\" + 0.005*\"couscous\" + 0.004*\"omar\" + 0.003*\"algerian\" + '\n",
      "  '0.003*\"bastilla\" + 0.003*\"north\" + 0.002*\"african\" + 0.002*\"merguez\" + '\n",
      "  '0.002*\"french\" + 0.002*\"brick\" + 0.002*\"feature\" + 0.002*\"warka\" + '\n",
      "  '0.002*\"lemonwhat\" + 0.001*\"algeria\" + 0.001*\"maghreb\" + 0.001*\"clay\" + '\n",
      "  '0.001*\"chicken\" + 0.001*\"californian\" + 0.001*\"moroccan\" + '\n",
      "  '0.001*\"powdered\"'),\n",
      " (16,\n",
      "  '0.008*\"restaurant\" + 0.006*\"like\" + 0.005*\"dish\" + 0.004*\"chef\" + '\n",
      "  '0.004*\"good\" + 0.004*\"menu\" + 0.003*\"food\" + 0.003*\"bar\" + 0.003*\"flavor\" + '\n",
      "  '0.003*\"come\" + 0.003*\"serve\" + 0.003*\"meal\" + 0.003*\"not\" + 0.003*\"course\" '\n",
      "  '+ 0.003*\"new_york\" + 0.002*\"city\" + 0.002*\"time\" + 0.002*\"sweet\" + '\n",
      "  '0.002*\"kitchen\" + 0.002*\"sauce\"'),\n",
      " (17,\n",
      "  '0.005*\"barbecue\" + 0.004*\"like\" + 0.004*\"meat\" + 0.004*\"serve\" + '\n",
      "  '0.004*\"qui\" + 0.003*\"pelmeni\" + 0.003*\"franklin\" + 0.003*\"good\" + '\n",
      "  '0.003*\"pork\" + 0.003*\"austin\" + 0.003*\"frenchette\" + 0.003*\"line\" + '\n",
      "  '0.003*\"potato\" + 0.003*\"chicken\" + 0.003*\"vegetable\" + 0.002*\"goat\" + '\n",
      "  '0.002*\"smoke\" + 0.002*\"restaurant\" + 0.002*\"duck\" + 0.002*\"include\"'),\n",
      " (18,\n",
      "  '0.007*\"restaurant\" + 0.007*\"like\" + 0.005*\"dish\" + 0.004*\"good\" + '\n",
      "  '0.004*\"chef\" + 0.004*\"menu\" + 0.003*\"lamb\" + 0.003*\"food\" + 0.003*\"salad\" + '\n",
      "  '0.003*\"chinese\" + 0.003*\"table\" + 0.003*\"lunch\" + 0.003*\"not\" + 0.003*\"egg\" '\n",
      "  '+ 0.003*\"white\" + 0.003*\"include\" + 0.002*\"open\" + 0.002*\"place\" + '\n",
      "  '0.002*\"green\" + 0.002*\"come\"'),\n",
      " (19,\n",
      "  '0.006*\"menu\" + 0.006*\"like\" + 0.005*\"dish\" + 0.005*\"restaurant\" + '\n",
      "  '0.004*\"good\" + 0.004*\"indian\" + 0.004*\"curry\" + 0.004*\"meat\" + 0.004*\"beer\" '\n",
      "  '+ 0.003*\"chef\" + 0.003*\"flavor\" + 0.003*\"sauce\" + 0.003*\"city\" + '\n",
      "  '0.003*\"bar\" + 0.002*\"new\" + 0.002*\"course\" + 0.002*\"food\" + '\n",
      "  '0.002*\"vegetable\" + 0.002*\"come\" + 0.002*\"room\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shile\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\numpy\\core\\fromnumeric.py:56: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>...</th>\n",
       "      <th>Topic11</th>\n",
       "      <th>Topic12</th>\n",
       "      <th>Topic13</th>\n",
       "      <th>Topic14</th>\n",
       "      <th>Topic15</th>\n",
       "      <th>Topic16</th>\n",
       "      <th>Topic17</th>\n",
       "      <th>Topic18</th>\n",
       "      <th>Topic19</th>\n",
       "      <th>Automated_topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092917</td>\n",
       "      <td>0.046725</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.312709</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.005823</td>\n",
       "      <td>0.541478</td>\n",
       "      <td>Topic19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>Topic16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.630613</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.368757</td>\n",
       "      <td>Topic16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.184576</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.601083</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.213741</td>\n",
       "      <td>Topic16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.119272</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.880161</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>Topic16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic0    Topic1    Topic2    Topic3    Topic4    Topic5    Topic6  \\\n",
       "0  0.000022  0.000023  0.000025  0.000025  0.000022  0.000024  0.000021   \n",
       "1  0.000028  0.000029  0.000032  0.000031  0.000028  0.000030  0.000026   \n",
       "2  0.000030  0.000031  0.000034  0.000033  0.000030  0.000032  0.000028   \n",
       "3  0.000030  0.000031  0.000034  0.184576  0.000030  0.000032  0.000028   \n",
       "4  0.000027  0.000027  0.000030  0.000029  0.000027  0.000028  0.000025   \n",
       "\n",
       "     Topic7    Topic8    Topic9         ...           Topic11   Topic12  \\\n",
       "0  0.000021  0.000025  0.000023         ...          0.092917  0.046725   \n",
       "1  0.000027  0.000032  0.000029         ...          0.000075  0.000043   \n",
       "2  0.000028  0.000034  0.000031         ...          0.000081  0.000046   \n",
       "3  0.000029  0.000034  0.000032         ...          0.000081  0.000047   \n",
       "4  0.000025  0.000030  0.000028         ...          0.000072  0.000041   \n",
       "\n",
       "    Topic13   Topic14   Topic15   Topic16   Topic17   Topic18   Topic19  \\\n",
       "0  0.000027  0.000025  0.000021  0.312709  0.000022  0.005823  0.541478   \n",
       "1  0.000034  0.000032  0.000026  0.999370  0.000027  0.000033  0.000042   \n",
       "2  0.000036  0.000034  0.000028  0.630613  0.000029  0.000035  0.368757   \n",
       "3  0.000037  0.000034  0.000028  0.601083  0.000029  0.000035  0.213741   \n",
       "4  0.119272  0.000030  0.000025  0.880161  0.000026  0.000031  0.000040   \n",
       "\n",
       "   Automated_topic_id  \n",
       "0             Topic19  \n",
       "1             Topic16  \n",
       "2             Topic16  \n",
       "3             Topic16  \n",
       "4             Topic16  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df = lda_model.get_document_topics(corpus,minimum_probability=0)\n",
    "lda_df = pd.DataFrame(list(lda_df))\n",
    "num_topics = lda_model.num_topics\n",
    "lda_df.columns = ['Topic'+str(i) for i in range(num_topics)]\n",
    "for i in range(len(lda_df.columns)):\n",
    "    lda_df.iloc[:,i]=lda_df.iloc[:,i].apply(lambda x: x[1])\n",
    "lda_df['Automated_topic_id'] =lda_df.apply(lambda x: np.argmax(x),axis=1)\n",
    "lda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:45:36: -8.840 per-word bound, 458.3 perplexity estimate based on a held-out corpus of 395 documents with 275313 words\n",
      "INFO - 17:45:36: using ParallelWordOccurrenceAccumulator(processes=11, batch_size=64) to estimate probabilities from sliding windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.840122531191453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:45:48: 1 batches submitted to accumulate stats from 64 documents (41855 virtual)\n",
      "INFO - 17:45:48: 2 batches submitted to accumulate stats from 128 documents (77143 virtual)\n",
      "INFO - 17:45:48: 3 batches submitted to accumulate stats from 192 documents (118880 virtual)\n",
      "INFO - 17:45:48: 4 batches submitted to accumulate stats from 256 documents (157333 virtual)\n",
      "INFO - 17:45:49: 5 batches submitted to accumulate stats from 320 documents (194634 virtual)\n",
      "INFO - 17:45:49: 6 batches submitted to accumulate stats from 384 documents (225682 virtual)\n",
      "INFO - 17:45:49: 7 batches submitted to accumulate stats from 448 documents (232258 virtual)\n",
      "INFO - 17:45:51: 11 accumulators retrieved from output queue\n",
      "INFO - 17:45:51: accumulated word occurrence stats for 232258 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score using c_v:  0.3263054753291283\n",
      "\n",
      "Coherence Score using u_mass:  -1.4572390267580078\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity, a measure of how good the model is. lower the better.\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "# Compute Coherence Score for lda model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_list, dictionary=words, coherence='c_v')\n",
    "coherence_lda_c_v = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score using c_v: ', coherence_lda_c_v)\n",
    "# Compute Coherence Score for lda model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_list, dictionary=words, coherence='u_mass')\n",
    "coherence_lda_u_mass = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score using u_mass: ', coherence_lda_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis' has no attribute 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9da5900655de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyLDAvis' has no attribute 'gensim'"
     ]
    }
   ],
   "source": [
    "a=pyLDAvis.gensim.prepare(lda_model, corpus, words)\n",
    "pyLDAvis.show(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
